











## FTS (*Full Text Search*) and related: SOLR/Lucene et al: document content search

We'll be using SOLR mostly, but here might be some interface libraries and some interesting alternatives and partial solutions towards producing effective inverted indexes, e.g. compressed bit/integer arrays, radix tries, approximate index search matchers, etc.:

- **abseil-cpp** [ğŸ“](./abseil-cpp) [ğŸŒ](https://github.com/GerHobbelt/abseil-cpp) -- a collection of C++ code (compliant to C++11) designed to augment the C++ standard library.
- **adaptive_clustering** [ğŸ“](./adaptive_clustering) [ğŸŒ](https://github.com/GerHobbelt/adaptive_clustering) -- a lightweight and accurate point cloud clustering method from the paper Online learning for 3D LiDAR-based human detection: Experimental analysis of point cloud clustering and classification methods, Zhi Yan and Tom Duckett and Nicola Bellotto, 2019.
- **adaptiveqf** [ğŸ“](./adaptiveqf) [ğŸŒ](https://github.com/GerHobbelt/adaptiveqf) -- [Adaptive Quotient Filter (AQF)](https://arxiv.org/abs/2107.02866) supports approximate membership testing and counting the occurrences of items in a data set. Like other AMQs, the AQF has a chance for false positives during queries. However, the AQF has the ability to adapt to false positives after they have occurred so they are not repeated. At the same time, the AQF maintains the benefits of a quotient filter, as it is small and fast, has good locality of reference, scales out of RAM to SSD, and supports deletions, counting, resizing, merging, and highly concurrent access.
- **adaptive-radix-tree** [ğŸ“](./adaptive-radix-tree) [ğŸŒ](https://github.com/GerHobbelt/adaptive-radix-tree) -- implements the Adaptive Radix Tree (ART), as proposed by Leis et al. ART, which is a trie based data structure, achieves its performance, and space efficiency, by compressing the tree both vertically, i.e., if a node has no siblings it is merged with its parent, and horizontally, i.e., uses an array which grows as the number of children increases. Vertical compression reduces the tree height and horizontal compression decreases a nodeâ€™s size.
- **annoy** [ğŸ“](./annoy) [ğŸŒ](https://github.com/GerHobbelt/annoy) -- ANNOY (<b>A</b>pproximate <b>N</b>earest <b>N</b>eighbors <b>O</b>h <b>Y</b>eah) is a C++ library to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are `mmap`-ped into memory so that many processes may share the same data. ANNOY is almost as fast as the fastest libraries, but what really sets Annoy apart is: it has the ability to use static files as indexes, enabling you to share an index across processes. ANNOY also decouples creating indexes from loading them, so you can pass around indexes as files and map them into memory quickly. ANNOY tries to minimize its memory footprint: the indexes are quite small. This is useful when you want to find nearest neighbors using multiple CPU's. Spotify uses ANNOY for music recommendations.
- **arangodb** [ğŸ“](./arangodb) [ğŸŒ](https://github.com/GerHobbelt/arangodb) -- a scalable open-source multi-model database natively supporting graph, document and search. All supported data models & access patterns can be combined in queries allowing for maximal flexibility.
- **ArborX** [ğŸ“](./ArborX) [ğŸŒ](https://github.com/GerHobbelt/ArborX) -- a library designed to provide performance portable algorithms for geometric search, similarly to nanoflann and Boost Geometry.
- **BCF-cuckoo-index** [ğŸ“](./BCF-cuckoo-index) [ğŸŒ](https://github.com/GerHobbelt/BCF) -- Better Choice Cuckoo Filter (BCF) is an efficient approximate set representation data structure. Different from the standard Cuckoo Filter (CF), BCF leverages the principle of the power of two choices to select the better candidate bucket during insertion. BCF reduces the average number of relocations of the state-of-the-art CF by 35%.
  
  - **left-for-dead**; reason: has some GCC + Linux specific coding constructs: intrinsics + Linux-only API calls, which increase the cost of porting.

- **binary_search** [ğŸ“](./binary_search) [ğŸŒ](https://github.com/GerHobbelt/binary_search) -- includes both the most commonly used binary search variant, as first published by Hermann Bottenbruch in 1962, plus several novel variants with improved performance. The most notable variant, the monobound binary search, executes two to four times faster than the standard binary search on arrays smaller than 1 million 32 bit integers: Boundless Binary Search, Doubletapped Binary Search, Monobound Binary Search, Tripletapped Binary Search, Quaternary Binary Search, Monobound Interpolated Binary Search, Adaptive Binary Search.
- **Bi-Sent2Vec** [ğŸ“](./Bi-Sent2Vec) [ğŸŒ](https://github.com/GerHobbelt/Bi-Sent2Vec) -- provides cross-lingual numerical representations (features) for words, short texts, or sentences, which can be used as input to any machine learning task with applications geared towards cross-lingual word translation, cross-lingual sentence retrieval as well as cross-lingual downstream NLP tasks. The library is a cross-lingual extension of [Sent2Vec](https://github.com/epfml/sent2vec). Bi-Sent2Vec vectors are also well suited to monolingual tasks as indicated by a marked improvement in the monolingual quality of the word embeddings. (For more details, see [paper](https://arxiv.org/abs/1912.12481))
- **BitFunnel** [ğŸ“](./BitFunnel) [ğŸŒ](https://github.com/GerHobbelt/BitFunnel) -- the BitFunnel index used by [Bing's](http://www.bing.com) super-fresh, news, and media indexes. The algorithm is described in [BitFunnel: Revisiting Signatures for Search](https://dl.acm.org/doi/pdf/10.1145/3077136.3080789).
- **bitrush-index** [ğŸ“](./bitrush-index) [ğŸŒ](https://github.com/GerHobbelt/bitrush-index) -- provides a serializable bitmap index able to index millions values/sec on a single thread. By default this library uses [ozbcbitmap] but if you want you can also use another compressed/uncrompressed bitmap. Only equality-queries (A = X) are supported.
- **blitsort** [ğŸ“](./blitsort) [ğŸŒ](https://github.com/GerHobbelt/blitsort) -- Blitsort is an in-place rotate quick/merge sort based on the stable out-of-place merge sort [quadsort](https://github.com/scandum/quadsort), stable out-of-place quicksort [fluxsort](https://github.com/scandum/fluxsort), and unstable in-place [crumsort](https://github.com/scandum/crumsort).
- **bloom** [ğŸ“](./bloom) [ğŸŒ](https://github.com/GerHobbelt/bloom) -- C++ Bloom Filter Library, which offers optimal parameter selection based on expected false positive rate, union, intersection and difference operations between bloom filters and compression of in-use table (increase of false positive probability vs space).
- **completesearch** [ğŸ“](./completesearch) [ğŸŒ](https://github.com/GerHobbelt/completesearch) -- a fast and interactive search engine for *context-sensitive prefix search* on a given collection of documents. It does not only provide search results, like a regular search engine, but also completions for the last (maybe only partially typed) query word that lead to a hit.
- **Containers** [ğŸ“](./Containers) [ğŸŒ](https://github.com/GerHobbelt/Containers) -- a library of associative array data structures implemented as binary trees in C: `bstree`: a binary search tree; `dstree`: a digital search tree, `trie`: a Fredkin tree, `critbit`: a crit-bit tree and `patricia`: a PATRICIA trie.
- **cqf** [ğŸ“](./cqf) [ğŸŒ](https://github.com/GerHobbelt/cqf) -- [A General-Purpose Counting Filter: Counting Quotient Filter (CQF)](https://dl.acm.org/doi/10.1145/3035918.3035963) supports approximate membership testing and counting the occurrences of items in a data set. This general-purpose AMQ is small and fast, has good locality of reference, scales out of RAM to SSD, and supports deletions, counting (even on skewed data sets), resizing, merging, and highly concurrent access.
- **CRoaring** [ğŸ“](./CRoaring) [ğŸŒ](https://github.com/GerHobbelt/CRoaring) -- portable Roaring bitmaps in C (and C++). Bitsets, also called bitmaps, are commonly used as fast data structures. Unfortunately, they can use too much memory. To compensate, we often use compressed bitmaps. Roaring bitmaps are compressed bitmaps which tend to outperform conventional compressed bitmaps such as WAH, EWAH or Concise. They are used by several major systems such as Apache Lucene and derivative systems such as Solr and Elasticsearch, etc.. The CRoaring library is used in several systems such as Apache Doris.
- **crumsort** [ğŸ“](./crumsort) [ğŸŒ](https://github.com/GerHobbelt/crumsort) -- is a hybrid quicksort / mergesort algorithm. The sort is in-place, unstable, adaptive, branchless, and has exceptional performance.
- **cuckoofilter** [ğŸ“](./cuckoofilter) [ğŸŒ](https://github.com/GerHobbelt/cuckoofilter) -- a key-value filter using cuckoo hashing, a Bloom filter replacement for approximated set-membership queries. While Bloom filters are well-known space-efficient data structures to serve queries like "if item x is in a set?", they do not support deletion. Their variances to enable deletion (like counting Bloom filters) usually require much more space.  Cuckoo ï¬lters provide the ï¬‚exibility to add and remove items dynamically. A cuckoo filter is based on cuckoo hashing (and therefore named as cuckoo filter).  It is essentially a cuckoo hash table storing each key's fingerprint. Cuckoo hash tables can be highly compact, thus a cuckoo filter could use less space than conventional Bloom ï¬lters, for applications that require low false positive rates (< 3%).
- **Cuckoo_Filter_simple** [ğŸ“](./Cuckoo_Filter_simple) [ğŸŒ](https://github.com/GerHobbelt/Cuckoo_Filter) -- a key-value filter using cuckoo hashing, substituting for bloom filter.
- **cuckoo-index** [ğŸ“](./cuckoo-index) [ğŸŒ](https://github.com/GerHobbelt/cuckoo-index) -- Cuckoo Index (CI) is a lightweight secondary index structure that represents the many-to-many relationship between keys and partitions of columns in a highly space-efficient way. CI associates variable-sized fingerprints in a Cuckoo filter with compressed bitmaps indicating qualifying partitions. The problem of finding all partitions that possibly contain a given lookup key is traditionally solved by maintaining one filter (e.g., a Bloom filter) per partition that indexes all unique key values contained in this partition. To identify all partitions containing a key, we would need to probe all per-partition filters (which could be many). Depending on the storage medium, a false positive there can be very expensive. Furthermore, secondary columns typically contain many duplicates (also across partitions). Cuckoo Index (CI) addresses these drawbacks of per-partition filters. (It must know all keys at build time, though.)
- **dablooms** [ğŸ“](./dablooms) [ğŸŒ](https://github.com/GerHobbelt/dablooms) -- a Scalable, Counting, Bloom Filter demonstrating a novel Bloom filter implementation that can scale, and provide not only the addition of new members, but reliable removal of existing members.
- **DCF-cuckoo-index** [ğŸ“](./DCF-cuckoo-index) [ğŸŒ](https://github.com/GerHobbelt/DCF) -- the Dynamic Cuckoo Filter (DCF) is an efficient approximate membership test data structure. Different from the classic Bloom filter and its variants, DCF is especially designed for highly dynamic datasets and supports extending and reducing its capacity. The DCF design is the first to achieve both reliable item deletion and flexibly extending/reducing for approximate set representation and membership testing. DCF outperforms the state-of-the-art DBF designs in both speed and memory consumption.
- **edit-distance** [ğŸ“](./edit-distance) [ğŸŒ](https://github.com/GerHobbelt/editdistance) -- a fast implementation of the edit distance (Levenshtein distance). The algorithm used in this library is proposed by _Heikki HyyrÃ¶, "Explaining and extending the bit-parallel approximate string matching algorithm of Myers", (2001) <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.19.7158&rep=rep1&type=pdf>_.
- **EWAHBoolArray** [ğŸ“](./EWAHBoolArray) [ğŸŒ](https://github.com/GerHobbelt/EWAHBoolArray) -- a C++ compressed bitset data structure (also called bitset or bit vector). It supports several word sizes by a template parameter (16-bit, 32-bit, 64-bit). You should expect the 64-bit word-size to provide better performance, but higher memory usage, while a 32-bit word-size might compress a bit better, at the expense of some performance.
- **eytzinger** [ğŸ“](./eytzinger) [ğŸŒ](https://github.com/GerHobbelt/eytzinger) -- `fixed_eytzinger_map` is a free implementation of Eytzingerâ€™s layout, in a form of an STL-like generic associative container, broadly compatible with a well-established access patterns. An Eytzinger map, or BFS(breadth-first search) map, places elements in a lookup order, which leads to a better memory locality. In practice, such container can outperform searching in sorted arrays, like `boost::flat_map`, due to less cache misses made in a lookup process. In comparison with RB-based trees, like `std::map`, lookup in Eytzinger map can be multiple times faster. Some comparison graphs are [given here](https://kazakov.life/2017/03/06/cache-friendly-associative-container/).
- **fastfilter_cpp** [ğŸ“](./fastfilter_cpp) [ğŸŒ](https://github.com/GerHobbelt/fastfilter_cpp) -- Fast Filter: Fast approximate membership filter implementations (C++, research library)
- **fluxsort** [ğŸ“](./fluxsort) [ğŸŒ](https://github.com/GerHobbelt/fluxsort) -- is a stable quicksort / mergesort hybrid algorithm. The sort is stable, adaptive, branchless, and has exceptional performance.
- **FM-fast-match** [ğŸ“](./FM-fast-match) [ğŸŒ](https://github.com/GerHobbelt/FAsT-Match) -- FAsT-Match: a port of the Fast Affine Template Matching algorithm (Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan, CVPR 2013, Portland)
- **forestdb** [ğŸ“](./forestdb) [ğŸŒ](https://github.com/GerHobbelt/forestdb) -- a key-value storage engine whos main index structure is built from [Hierarchical B+-Tree based Trie](http://db.csail.mit.edu/sigmod11contest/sigmod_2011_contest_poster_jungsang_ahn.pdf), called HB+-Trie. [ForestDB paper](https://www.computer.org/csdl/trans/tc/preprint/07110563.pdf) has been published in IEEE Transactions on Computers. Compared with traditional B+-Tree based storage engines, ForestDB shows significantly better read and write performance with less storage overhead. ForestDB has been tested on various server OS environments (Centos, Ubuntu, Mac OS x, Windows) and mobile OSs (iOS, Android).
- **fuzzy-match** [ğŸ“](./fuzzy-match) [ğŸŒ](https://github.com/GerHobbelt/fuzzy-match) -- `FuzzyMatch-cli` is a commandline utility allowing to compile FuzzyMatch indexes and use them to lookup fuzzy matches. Okapi BM25 prefiltering is available on branch [`bm25`](https://github.com/SYSTRAN/fuzzy-match/tree/bm25).
- **fxt** [ğŸ“](./fxt) [ğŸŒ](https://github.com/GerHobbelt/fxt) -- a large scale feature extraction tool for text-based machine learning.
- **groonga** [ğŸ“](./groonga) [ğŸŒ](https://github.com/GerHobbelt/groonga) -- an open-source fulltext search engine and column store.
- **hopscotch-map** [ğŸ“](./hopscotch-map) [ğŸŒ](https://github.com/GerHobbelt/hopscotch-map) -- a C++ implementation of a fast hash map and hash set using hopscotch hashing and open-addressing to resolve collisions. It is a cache-friendly data structure offering better performances than `std::unordered_map` in most cases and is closely similar to `google::dense_hash_map` while using less memory and providing more functionalities.
- **iceberghashtable** [ğŸ“](./iceberghashtable) [ğŸŒ](https://github.com/GerHobbelt/iceberghashtable) -- [IcebergDB: High Performance Hash Tables Through Stability and Low Associativity](https://arxiv.org/abs/2210.04068) is a fast, concurrent, and resizeable hash table implementation. It supports insertions, deletions and queries for 64-bit keys and values.
- **iresearch** [ğŸ“](./iresearch) [ğŸŒ](https://github.com/GerHobbelt/iresearch) -- the IResearch search engine is meant to be treated as a standalone index that is capable of both indexing and storing individual values verbatim. Indexed data is treated on a per-version/per-revision basis, i.e. existing data version/revision is never modified and updates/removals are treated as new versions/revisions of the said data. This allows for trivial multi-threaded read/write operations on the index. The index exposes its data processing functionality via a multi-threaded 'writer' interface that treats each document abstraction as a collection of fields to index and/or store. The index exposes its data retrieval functionality via 'reader' interface that returns records from an index matching a specified query. The queries themselves are constructed query trees built directly using the query building blocks available in the API. The querying infrastructure provides the capability of ordering the result set by one or more ranking/scoring implementations. The ranking/scoring implementation logic is plugin-based and lazy-initialized during runtime as needed, allowing for addition of custom ranking/scoring logic without the need to even recompile the IResearch library.
- **Jungle** [ğŸ“](./Jungle) [ğŸŒ](https://github.com/GerHobbelt/Jungle) -- an embedded key-value storage library, based on a combined index of [LSM-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree) and [copy-on-write (append-only) B+tree](https://www.usenix.org/legacy/events/lsf07/tech/rodeh.pdf). Please refer to our [paper](https://www.usenix.org/conference/hotstorage19/presentation/ahn). Jungle is specialized for building [replicated state machine](https://en.wikipedia.org/wiki/State_machine_replication) of consensus protocols such as [Paxos](https://en.wikipedia.org/wiki/Paxos_(computer_science)) or [Raft](https://raft.github.io/), by providing chronological ordering and lightweight persistent snapshot. It can be also used for building log store.
- **LDCF-hash** [ğŸ“](./LDCF-hash) [ğŸŒ](https://github.com/GerHobbelt/LDCF) -- The Logarithmic Dynamic Cuckoo Filter (LDCF) is an efficient approximate membership test data structure for dynamic big data sets. LDCF uses a novel multi-level tree structure and reduces the worst insertion and membership testing time from O(N) to O(1), while simultaneously reducing the memory cost of DCF as the cardinality of the set increases.
- **libart** [ğŸ“](./libart) [ğŸŒ](https://github.com/GerHobbelt/libart) -- provides the Adaptive Radix Tree or ART. The ART operates similar to a traditional radix tree but avoids the wasted space of internal nodes by changing the node size. It makes use of 4 node sizes (4, 16, 48, 256), and can guarantee that the overhead is no more than 52 bytes per key, though in practice it is much lower.
- **libbloom** [ğŸ“](./libbloom) [ğŸŒ](https://github.com/GerHobbelt/bloomd) -- a high-performance C server, exposing bloom filters and operations over them. The rate of false positives can be tuned to meet application demands, but reducing the error rate rapidly increases the amount of memory required for the representation. Example: Bloom filters enable you to represent 1MM items with a false positive rate of 0.1% in 2.4MB of RAM.
- **libbloomfilters** [ğŸ“](./libbloomfilters) [ğŸŒ](https://github.com/GerHobbelt/libbloomfilters) -- **libbf** is a C++11 library which implements various Bloom filters, including:
  
  - A^2
  - Basic
  - Bitwise
  - Counting
  - Spectral MI
  - Spectral RM
  - Stable

- **libchopshop** [ğŸ“](./libchopshop) [ğŸŒ](https://github.com/GerHobbelt/libchopshop) -- NLP/text processing with automated stop word detection and stemmer-based filtering. This library / toolkit is engineered to be able to provide **both** of the (often more or less disparate) n-gram token streams / vectors required for (1) initializing / training FTS databases, neural nets, etc. and (2) executing effective queries / matches on these engines.
- **libCSD** [ğŸ“](./libCSD) [ğŸŒ](https://github.com/GerHobbelt/libCSD) -- a C++ library providing some different techniques for managing string dictionaries in compressed space. These approaches are inspired on the paper: "Compressed String Dictionaries", Nieves R. Brisaboa, Rodrigo CÃ¡novas, Francisco Claude, Miguel A. MartÃ­nez-Prieto, and Gonzalo Navarro, 10th Symposium on Experimental Algorithms (SEA'2011), p.136-147, 2011.
- **libcuckoo** [ğŸ“](./libcuckoo) [ğŸŒ](https://github.com/GerHobbelt/libcuckoo) -- provides a high-performance, compact hash table that allows multiple concurrent reader and writer threads.
- **libhashish** [ğŸ“](./libhashish) [ğŸŒ](https://github.com/GerHobbelt/libhashish) -- non-cryptographic hash algorithms & various applications thereof (hash tables = dictionaries, bloom filters, ...)
- **libunibreak** [ğŸ“](./libunibreak) [ğŸŒ](https://github.com/GerHobbelt/libunibreak) -- an implementation of the line breaking and word breaking algorithms as described in (Unicode Standard Annex 14)[http://www.unicode.org/reports/tr14/] and (Unicode Standard Annex 29)[http://www.unicode.org/reports/tr29/].
- **ligra-graph** [ğŸ“](./ligra-graph) [ğŸŒ](https://github.com/GerHobbelt/ligra) -- LIGRA: a Lightweight Graph Processing Framework for Shared Memory; works on both uncompressed and compressed graphs and hypergraphs.
- [Manticore](https://manticoresearch.com/) -- while the userbase is much smaller than for the *Lucene Gang* (Lucene/SOLR/ES/OpenSearch), this still got me. Can't say exactly why. All the other Lucene/SOLR alternatives out there didn't appeal to me (old tech, slow dev, ...).
  
  - **manticore-columnar** [ğŸ“](./manticore-columnar) [ğŸŒ](https://github.com/GerHobbelt/columnar) -- Manticore Columnar Library is a column-oriented storage and secondary indexing library, aiming to provide **decent performance with low memory footprint at big data volume**. When used in combination with [Manticore Search](https://github.com/manticoresoftware/manticoresearch) it can be beneficial for those looking for:
    
    1. log analytics including rich free text search capabities (which is missing in e.g. [Clickhouse](https://github.com/ClickHouse/ClickHouse) - great tool for metrics analytics)
    2. faster / low resource consumption log/metrics analytics. Since the library and Manticore Search are both written in C++ with low optimizations in mind, in many cases the performance / RAM consumption is better than in Lucene / SOLR / Elasticsearch
    3. running log / metric analytics in docker / kubernetes. Manticore Search + the library can work with as little as 30 megabytes of RAM which Elasticsearch / Clickhouse can't. It also starts in less than a second or a few seconds in the worst case. Since the overhead is so little you can afford having more nodes of Manticore Search + the library than Elasticsearch. More nodes and quicker start means higher high availability and agility.
    4. powerful SQL for logs/metrics analytics and everything else [Manticore Search](https://github.com/manticoresoftware/manticoresearch) can give you
  
  - **manticore-plugins** [ğŸ“](./manticore-plugins) [ğŸŒ](https://github.com/GerHobbelt/manticore-plugins) -- Manticore Search plugins and UDFs (user defined functions) -- Manticore Search can be extended with help of plugins and custom functions (aka user defined functions or UDFs).
  - **manticoresearch** [ğŸ“](./manticoresearch) [ğŸŒ](https://github.com/GerHobbelt/manticoresearch) -- Manticore Search is an easy to use open source fast database for search. Good alternative for Elasticsearch. What distinguishes it from other solutions is:
    
    * It's very fast and therefore more cost-efficient than alternatives, for example Manticore is:
    * Modern MPP architecture and smart query parallelization capabilities allow to fully utilize all your CPU cores to **lower response time** as much as possible, when needed.
    * Powerful and fast full-text search which **works fine for small and big datasets**
    * Traditional **row-wise storage** for small, medium and big size datasets
    * **Columnar storage** support via the [Manticore Columnar Library](https://github.com/manticoresoftware/columnar/) for bigger datasets (much bigger than can fit in RAM)
    * Easy to use secondary indexes (you don't need to create them manually)
    * Cost-based optimizer for search queries
    * SQL-first: Manticore's **native syntax is SQL**. It speaks SQL over HTTP and uses the MySQL protocol (you can use your preferred MySQL client)
    * **JSON over HTTP**: to provide a more programmatic way to manage your data and schemas, Manticore provides a HTTP JSON protocol
    * Written fully in C++: **starts fast, doesn't take much RAM**, and low-level optimizations provide good performance
    * **Real-time inserts**: after an INSERT is made, the document is accessible immediately
    * [Interactive courses](https://play.manticoresearch.com/) for **easier learning**
    * **Built-in replication and load balancing**
    * **Can sync** from MySQL/PostgreSQL/ODBC/xml/csv out of the box
    * Not fully ACID-compliant, but **supports transactions and binlog** for safe writes

- **mitlm** [ğŸ“](./mitlm) [ğŸŒ](https://github.com/GerHobbelt/mitlm) -- the MIT Language Modeling Toolkit (MITLM) toolkit is a set of tools designed for the efficient estimation of statistical n-gram language models involving iterative parameter estimation.  It achieves much of its efficiency through the use of a compact vector representation of n-grams.
- **morton_filter** [ğŸ“](./morton_filter) [ğŸŒ](https://github.com/GerHobbelt/morton_filter) -- a [Morton filter](https://www.vldb.org/pvldb/vol11/p1041-breslow.pdf) -- a new approximate set membership data structure. A Morton filter is a modified cuckoo filter that is optimized for bandwidth-constrained systems. Morton filters use additional computation in order to reduce their off-chip memory traffic. Like a cuckoo filter, a Morton filter supports insertions, deletions, and lookup operations. It additionally adds high-throughput self-resizing, a feature of quotient filters, which allows a Morton filter to increase its capacity solely by leveraging its internal representation. This capability is in contrast to existing vanilla cuckoo filter implementations, which are static and thus require using a backing data structure that contains the full set of items to resize the filter. Morton filters can also be configured to use less memory than a cuckoo filter for the same error rate while simultaneously delivering insertion, deletion, and lookup throughputs that are, respectively, up to 15.5x, 1.3x, and 2.5x higher than a cuckoo filter. Morton filters in contrast to vanilla cuckoo filters do not require a power of two number of buckets but rather only a number that is a multiple of two. They also use fewer bits per item than a Bloom filter when the target false positive rate is less than around 1% to 3%.
- **mutable_rank_select** [ğŸ“](./mutable_rank_select) [ğŸŒ](https://github.com/GerHobbelt/mutable_rank_select) -- Rank/Select Queries over Mutable Bitmaps. Given a *mutable* bitmap `B[0..u)` where `n` bits are set, the *rank/select problem* asks for a data structure built from `B` that supports `rank(i)` (the number of bits set in `B[0..i]`, for 0 â‰¤ i < u), `select(i)` (the position of the i-th bit set, for 0 â‰¤ i < n), `flip(i)` (toggles `B[i]`, for 0 â‰¤ i < u) and `access(i)` (return `B[i]`, for 0 â‰¤ i < u). The input bitmap is partitioned into blocks and a tree index is built over them. The tree index implemented in the library is an optimized b-ary Segment-Tree with SIMD AVX2/AVX-512 instructions. You can test a block size of 256 or 512 bits, and various rank/select algorithms for the blocks such as broadword techniques, CPU intrinsics, and SIMD instructions.
- **nedtries** [ğŸ“](./nedtries) [ğŸŒ](https://github.com/GerHobbelt/nedtries) -- an in-place bitwise binary Fredkin trie algorithm which allows for near constant time insertions, deletions, finds, closest fit finds and iteration. On modern hardware it is approximately 50-100% faster than red-black binary trees, it handily beats even the venerable O(1) hash table for less than 3000 objects and it is barely slower than the hash table for 10000 objects. Past 10000 objects you probably ought to use a hash table though, and if you need nearest fit rather than close fit then red-black trees are still optimal.
- **OZBCBitmap** [ğŸ“](./OZBCBitmap) [ğŸŒ](https://github.com/GerHobbelt/OZBCBitmap) -- OZBC provides an efficent compressed bitmap to create bitmap indexes on high-cardinality columns. Bitmap indexes have traditionally been considered to work well for low-cardinality columns, which have a modest number of distinct values. The simplest and most common method of bitmap indexing on attribute A with K cardinality associates a bitmap with every attribute value V then the Vth bitmap rapresent the predicate A=V. This approach ensures an efficient solution for performing search but on high-cardinality attributes the size of the bitmap index increase dramatically. OZBC is a run-length-encoded hybrid compressed bitmap designed exclusively to create a bitmap indexes on L cardinality attributes where L>=16 and provide bitwise logical operations in running time complexity proportianl to the compressed bitmap size.
- **pfp-cst** [ğŸ“](./pfp-cst) [ğŸŒ](https://github.com/GerHobbelt/pfp-cst) -- Prefix-Free Parsing Compressed Suffix Tree is a compressed suffix tree, built on the prefix-free parsing of the text. If you use the PFP-CST in your research, please cite: Christina Boucher, OndÅ™ej Cvacho, Travis Gagie, Jan Holub, Giovanni Manzini, Gonzalo Navarro, and Massimiliano Rossi . *"PFP Compressed Suffix Tree"*, In Proc. of the SIAM Symposium onAlgorithm Engineering and Experiments (ALENEX21), pp. 60-72. (2021).
- **PGM-index** [ğŸ“](./PGM-index) [ğŸŒ](https://github.com/GerHobbelt/PGM-index) -- the Piecewise Geometric Model index (PGM-index) is a data structure that enables fast lookup, predecessor, range searches and updates in arrays of billions of items using orders of magnitude less space than traditional indexes while providing the same worst-case query time guarantees.
- **pg_similarity** [ğŸ“](./pg_similarity) [ğŸŒ](https://github.com/GerHobbelt/pg_similarity) -- **pg\_similarity** is an extension to support similarity queries on [PostgreSQL](http://www.postgresql.org/). The implementation is tightly integrated in the RDBMS in the sense that it defines operators so instead of the traditional operators (`=` and `<>`) you can use `~~~` and `~!~` (any of these operators represents a similarity function).
- **piposort** [ğŸ“](./piposort) [ğŸŒ](https://github.com/GerHobbelt/piposort) -- a stable top-down adaptive branchless merge sort named piposort. It is intended as a simplified [quadsort](https://github.com/scandum/quadsort) with reduced adaptivity, but a great reduction in lines of code and overall complexity. The name stands for ping-pong.
- **pisa** [ğŸ“](./pisa) [ğŸŒ](https://github.com/GerHobbelt/pisa) -- a text search engine able to run on large-scale collections of documents. It allows researchers to experiment with state-of-the-art techniques, allowing an ideal environment for rapid development. PISA is a text search engine, though the "PISA Project" is a set of tools that help experiment with indexing and query processing. Given a text collection, PISA can build an inverted index over this corpus, allowing the corpus to be searched. The inverted index, put simply, is an efficient data structure that represents the document corpus by storing a list of documents for each unique term (see here). At query time, PISA stores its index in main memory for rapid retrieval.
- **pisa_formatter** [ğŸ“](./pisa_formatter) [ğŸŒ](https://github.com/GerHobbelt/pisa_formatter) -- converts list of documents to the [pisa-engine](https://github.com/pisa-engine/pisa) binary format: {.docs, .freqs, .sizes}. Its input should be a text file where each line is a document.  Each document starts with the document name (which should not have whitespaces) followed by a list of ascii terms separated by whitespaces which define the document. This also generates a binary .terms file which has the information to convert from term to index and is used by the query_transformer executable. This file stores all the unique terms from all the documents.
- **pmemkv** [ğŸ“](./pmemkv) [ğŸŒ](https://github.com/GerHobbelt/pmemkv) -- `pmemkv` is a local/embedded key-value datastore optimized for persistent memory. Rather than being tied to a single language or backing implementation, `pmemkv` provides different options for language bindings and storage engines.
- **pmemkv-bench** [ğŸ“](./pmemkv-bench) [ğŸŒ](https://github.com/GerHobbelt/pmemkv-bench) -- benchmark for [libpmemkv](https://github.com/pmem/pmemkv/) and its underlying libraries, based on [leveldb's db_bench](https://github.com/google/leveldb). The `pmemkv_bench` utility provides some standard read, write & remove benchmarks. It's based on the `db_bench` utility included with LevelDB and RocksDB, although the list of supported parameters is slightly different.
- **poplar-trie** [ğŸ“](./poplar-trie) [ğŸŒ](https://github.com/GerHobbelt/poplar-trie) -- a C++17 library of a memory-efficient associative array whose keys are strings. The data structure is based on a dynamic path-decomposed trie (DynPDT) described in the paper, Shunsuke Kanda, Dominik KÃ¶ppl, Yasuo Tabei, Kazuhiro Morita, and Masao Fuketa: [Dynamic Path-decomposed Tries](https://arxiv.org/abs/1906.06015), *ACM Journal of Experimental Algorithmics (JEA)*, *25*(1): 1â€“28, 2020. Poplar-trie is a memory-efficient updatable associative array implementation which maps key strings to values of any type like `std::map<std::string,anytype>`. DynPDT is composed of two structures: dynamic trie and node label map (NLM) structures.
- **PruningRadixTrie** [ğŸ“](./PruningRadixTrie) [ğŸŒ](https://github.com/GerHobbelt/PruningRadixTrie) -- a 1000x faster Radix trie for prefix search & auto-complete, the PruningRadixTrie is a novel data structure, derived from a radix trie - but 3 orders of magnitude faster. A **Pruning Radix trie** is a novel Radix trie algorithm, that allows pruning of the Radix trie and early termination of the lookup. In many cases, we are not interested in a complete set of all children for a given prefix, but only in the top-k most relevant terms. Especially for short prefixes, this results in a **massive reduction of lookup time** for the top-10 results. On the other hand, a complete result set of millions of suggestions wouldn't be helpful at all for autocompletion. The lookup acceleration is achieved by storing in each node the maximum rank of all its children. By comparing this maximum child rank with the lowest rank of the results retrieved so far, we can heavily prune the trie and do early termination of the lookup for non-promising branches with low child ranks.
- **quadsort** [ğŸ“](./quadsort) [ğŸŒ](https://github.com/GerHobbelt/quadsort) -- a high performance stable bottom-up adaptive branchless merge sort algorithm.
- **radix_tree** [ğŸ“](./radix_tree) [ğŸŒ](https://github.com/GerHobbelt/radix_tree) -- STL like container of radix tree in C++.
- **rankselect** [ğŸ“](./rankselect) [ğŸŒ](https://github.com/GerHobbelt/rankselect) -- space-efficient, high-performance rank & select structures on uncompressed bit sequences.
- **rax** [ğŸ“](./rax) [ğŸŒ](https://github.com/GerHobbelt/rax) -- an ANSI C radix tree implementation initially written to be used in a specific place of Redis in order to solve a performance problem, but immediately converted into a stand alone project to make it reusable for Redis itself, outside the initial intended application, and for other projects as well. The primary goal was to find a suitable balance between performances and memory usage, while providing a fully featured implementation of radix trees that can cope with many different requirements.
- **RectangleBinPack** [ğŸ“](./RectangleBinPack) [ğŸŒ](https://github.com/GerHobbelt/RectangleBinPack) -- the source code used in "A Thousand Ways to Pack the Bin - A Practical Approach to Two-Dimensional Rectangle Bin Packing." The code can be
- **RoaringBitmap** [ğŸ“](./RoaringBitmap) [ğŸŒ](https://github.com/GerHobbelt/RoaringBitmap) -- Roaring bitmaps are compressed bitmaps which tend to outperform conventional compressed bitmaps such as WAH, EWAH or Concise. In some instances, roaring bitmaps can be hundreds of times faster and they often offer significantly better compression. They can even be faster than uncompressed bitmaps.
- **robin-hood-hashing** [ğŸ“](./robin-hood-hashing) [ğŸŒ](https://github.com/GerHobbelt/robin-hood-hashing) -- robin_hood unordered map & set.
- **robin-map** [ğŸ“](./robin-map) [ğŸŒ](https://github.com/GerHobbelt/robin-map) -- a C++ implementation of a fast hash map and hash set using open-addressing and linear robin hood hashing with backward shift deletion to resolve collisions.
- **rollinghashcpp** [ğŸ“](./rollinghashcpp) [ğŸŒ](https://github.com/GerHobbelt/rollinghashcpp) -- randomized rolling hash functions in C++. This is a set of C++ classes implementing various recursive n-gram hashing techniques, also called rolling hashing (http://en.wikipedia.org/wiki/Rolling_hash), including Randomized Karp-Rabin (sometimes called Rabin-Karp), Hashing by Cyclic Polynomials (also known as Buzhash) and Hashing by Irreducible Polynomials.
- **RTree** [ğŸ“](./RTree) [ğŸŒ](https://github.com/GerHobbelt/RTree) -- R-Tree: a Dynamic Index Structure for Spatial Searching, implemented as a C++ template, generally compatible with the STL and Boost C++ libraries.
- **sent2vec** [ğŸ“](./sent2vec) [ğŸŒ](https://github.com/GerHobbelt/sent2vec) -- a tool and pre-trained models related to the [Bi-Sent2vec](https://arxiv.org/abs/1912.12481). The cross-lingual extension of Sent2Vec can be found [here](https://github.com/epfml/Bi-sent2vec). This library provides numerical representations (features) for words, short texts, or sentences, which can be used as input to any machine learning task.
- **sist2** [ğŸ“](./sist2) [ğŸŒ](https://github.com/GerHobbelt/sist2) -- sist2 (Simple incremental search tool) is a fast, low memory usage, multi-threaded application, which scans drives and directory trees, extracts text and metadata from common file types, generates thumbnails and comes with OCR support (with tesseract) and  Named-Entity Recognition (using pre-trained client-side tensorflow models).
- **sparsehash** [ğŸ“](./sparsehash) [ğŸŒ](https://github.com/GerHobbelt/sparsehash) -- fast (non-cryptographic) hash algorithms
- **sparse-map** [ğŸ“](./sparse-map) [ğŸŒ](https://github.com/GerHobbelt/sparse-map) -- a C++ implementation of a memory efficient hash map and hash set. It uses open-addressing with sparse quadratic probing. The goal of the library is to be the most memory efficient possible, even at low load factor, while keeping reasonable performances.
- **sparsepp** [ğŸ“](./sparsepp) [ğŸŒ](https://github.com/GerHobbelt/sparsepp) -- a fast, memory efficient hash map for C++. Sparsepp is derived from Google's excellent [sparsehash](https://github.com/sparsehash/sparsehash) implementation.
- **sqlite-fts5-snowball** [ğŸ“](./sqlite-fts5-snowball) [ğŸŒ](https://github.com/GerHobbelt/fts5-snowball) -- a simple extension for use with FTS5 within SQLite. It allows FTS5 to use Martin Porter's Snowball stemmers (libstemmer), which are available in several languages. Check http://snowballstem.org/ for more information about them.
- **sqlite_fts_tokenizer_chinese_simple** [ğŸ“](./sqlite_fts_tokenizer_chinese_simple) [ğŸŒ](https://github.com/GerHobbelt/simple) -- an extension of [sqlite3 fts5](https://www.sqlite.org/fts5.html) that supports Chinese and Pinyin. It fully provides a [solution to the multi-phonetic word problem of full-text retrieval on WeChat mobile terminal](https://cloud.tencent.com/developer/article/1198371): solution 4 in the article, very simple and efficient support for Chinese and Pinyin searches.
  
  On this basis we also support more accurate phrase matching through [cppjieba](https://github.com/yanyiwu/cppjieba). See the introduction article at https://www.wangfenjin.com/posts/simple-jieba-tokenizer/

- **SQLiteHistograms** [ğŸ“](./SQLiteHistograms) [ğŸŒ](https://github.com/GerHobbelt/SQLiteHistograms) -- an SQLite extension library for creating histogram tables, tables of ratio between histograms and interpolation tables of scatter point tables.
- **sqlite-stats** [ğŸ“](./sqlite-stats) [ğŸŒ](https://github.com/GerHobbelt/sqlite-stats) -- provides common statistical functions for SQLite.
- **SZ** [ğŸ“](./SZ) [ğŸŒ](https://github.com/GerHobbelt/SZ) -- SZ2: Error-bounded Lossy Compressor for HPC Data, (C) 2016-2022 by Mathematics and Computer Science (MCS), Argonne National Laboratory.
- **SZ3** [ğŸ“](./SZ3) [ğŸŒ](https://github.com/GerHobbelt/SZ3) -- SZ3: A Modular Error-bounded Lossy Compression Framework for Scientific Datasets, 2016 by Mathematics and Computer Science (MCS), Argonne National Laboratory.
- **tensorstore** [ğŸ“](./tensorstore) [ğŸŒ](https://github.com/GerHobbelt/tensorstore) -- TensorStore is an open-source C++ and Python software library designed for storage and manipulation of large multi-dimensional arrays.
- **typesense** [ğŸ“](./typesense) [ğŸŒ](https://github.com/GerHobbelt/typesense) -- a fast, typo-tolerant search engine for building delightful search experiences. Open Source alternative to Algolia and an Easier-to-Use alternative to ElasticSearch. âš¡ğŸ”âœ¨ Fast, typo tolerant, in-memory fuzzy Search Engine for building delightful search experiences.
- **unordered_dense** [ğŸ“](./unordered_dense) [ğŸŒ](https://github.com/GerHobbelt/unordered_dense) -- `ankerl::unordered_dense::{map, set}` is a fast & densely stored hashmap and hashset based on robin-hood backward shift deletion for C++17 and later. The classes `ankerl::unordered_dense::map` and `ankerl::unordered_dense::set` are (almost) drop-in replacements of `std::unordered_map` and `std::unordered_set`. While they don't have as strong iterator / reference stability guaranties, they are typically *much* faster. Additionally, there are `ankerl::unordered_dense::segmented_map` and `ankerl::unordered_dense::segmented_set` with lower peak memory usage. and stable iterator/references on insert.
- **upscaledb** [ğŸ“](./upscaledb) [ğŸŒ](https://github.com/GerHobbelt/hamsterdb) -- a.k.a. hamsterdb: a thread-safe key/value database engine. It supports a B+Tree index structure, uses memory mapped I/O (if available), fast Cursors and variable length keys and can create In-Memory Databases.
- **vqf** [ğŸ“](./vqf) [ğŸŒ](https://github.com/GerHobbelt/vqf) -- Vector Quotient Filters: Overcoming the Time/Space Trade-Off in Filter Design. The VQF supports approximate membership testing of items in a data set. The VQF is based on Robin Hood hashing, like the quotient filter, but uses power-of-two-choices hashing to reduce the variance of runs, and thus offers consistent, high throughput across load factors. Power-of-two-choices hashing also makes it more amenable to concurrent updates.
- **wolfsort** [ğŸ“](./wolfsort) [ğŸŒ](https://github.com/GerHobbelt/wolfsort) -- a stable adaptive hybrid bucket / quick / merge / drop sort named wolfsort. The bucket sort, forming the core of wolfsort, is not a comparison sort, so wolfsort can be considered a member of the radix-sort family. Quicksort and mergesort are well known. Dropsort gained popularity after it was reinvented as Stalin sort.
- **xcdat** [ğŸ“](./xcdat) [ğŸŒ](https://github.com/GerHobbelt/xcdat) -- a C++17 header-only library of a fast compressed string dictionary based on an improved double-array trie structure described in the paper: [Compressed double-array tries for string dictionaries supporting fast lookup](https://doi.org/10.1007/s10115-016-0999-8), *Knowledge and Information Systems*, 2017, available [here](https://kampersanda.github.io/pdf/KAIS2017.pdf).
- **xor-and-binary-fuse-filter** [ğŸ“](./xor-and-binary-fuse-filter) [ğŸŒ](https://github.com/GerHobbelt/xor_singleheader) -- XOR and Binary Fuse Filter library: Bloom filters are used to quickly check whether an element is part of a set. Xor filters and binary fuse filters are faster and more concise alternative to Bloom filters. They are also smaller than cuckoo filters. They are used in [production systems](https://github.com/datafuselabs/databend).
- **xsg** [ğŸ“](./xsg) [ğŸŒ](https://github.com/GerHobbelt/xsg) -- XOR [BST](https://en.wikipedia.org/wiki/Binary_search_tree) implementations are related to the [XOR linked list](https://en.wikipedia.org/wiki/XOR_linked_list), a [doubly linked list](https://en.wikipedia.org/wiki/Doubly_linked_list) variant, from where we borrow the idea about how links between nodes are to be implemented. Modest resource requirements and simplicity make XOR [scapegoat trees](https://en.wikipedia.org/wiki/Scapegoat_tree) stand out of the [BST](https://en.wikipedia.org/wiki/Binary_search_tree) crowd. All iterators (except `end()` iterators), but not references and pointers, are invalidated, after inserting or erasing from this XOR [scapegoat tree](https://en.wikipedia.org/wiki/Scapegoat_tree) implementation. You can dereference invalidated iterators, if they were not erased, but you cannot iterate with them. `end()` iterators are constant and always valid, but dereferencing them results in undefined behavior.
- **zvec** [ğŸ“](./zvec) [ğŸŒ](https://github.com/GerHobbelt/zvec) -- _zip_vector_ is a compressed variable length array that uses vectorized block codecs to compress and decompress integers using variable bit-width deltas. The integer block codecs are optimized for vector instruction sets using Google's Highway C++ library for portable SIMD/vector intrinsics.





	
----

ğŸ¡¸ [previous section](./0050-export-output-file-formats-text-formatting-etc.md)  |  ğŸ¡¹ [up](./0006-libraries-we-re-looking-at-for-this-intent.md)  |  ğŸ¡» [all (index)](./0093-libraries-in-this.md)  |  ğŸ¡º [next section](./0052-stemmers.md)
