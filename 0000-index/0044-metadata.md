

## metadata & text (OCR et al) -- language detect, suggesting fixes, ...

- **chewing_text_cud** [ğŸ“](./chewing_text_cud) [ğŸŒ](https://github.com/GerHobbelt/chewing_text_cud) -- a text processing / filtering library for use in NLP/search/content analysis research pipelines.
- **cld1-language-detect** [ğŸ“](./cld1-language-detect) [ğŸŒ](https://github.com/GerHobbelt/cld) -- the CLD (Compact Language Detection) library, extracted from the source code for Google's Chromium library. CLD1 probabilistically detects languages in Unicode UTF-8 text.
- **cld2-language-detect** [ğŸ“](./cld2-language-detect) [ğŸŒ](https://github.com/GerHobbelt/cld2) -- CLD2 probabilistically detects over 80 languages in Unicode UTF-8 text, either plain text or HTML/XML. For mixed-language input, CLD2 returns the top three languages found and their approximate percentages of the total text bytes.  Optionally, it also returns a vector of text spans with the language of each identified. The design target is web pages of at least 200 characters (about two sentences); CLD2 is not designed to do well on very short text.
- **cld3-language-detect** [ğŸ“](./cld3-language-detect) [ğŸŒ](https://github.com/GerHobbelt/cld3) -- CLD3 is a neural network model for language identification. The inference code extracts character ngrams from the input text and computes the fraction of times each of them appears. The model outputs BCP-47-style language codes, shown in the table below. For some languages, output is differentiated by script. Language and script names from Unicode CLDR.
- **compact_enc_det** [ğŸ“](./compact_enc_det) [ğŸŒ](https://github.com/GerHobbelt/compact_enc_det) -- Compact Encoding Detection (CED for short) is a library written in C++ that scans given raw bytes and detect the most likely text encoding.
- **cppjieba** [ğŸ“](./cppjieba) [ğŸŒ](https://github.com/GerHobbelt/cppjieba) -- the C++ version of the Chinese "Jieba" project:
  
  - Supports loading a custom user dictionary, using the '|' separator when multipathing or the ';' separator for separate, multiple, dictionaries.
  - Supports 'utf8' encoding.
  - The project comes with a relatively complete unit test, and the stability of the core function Chinese word segmentation (utf8) has been tested by the online environment.

- **cpp-unicodelib** [ğŸ“](./cpp-unicodelib) [ğŸŒ](https://github.com/GerHobbelt/cpp-unicodelib) -- a C++17 single-file header-only Unicode library.
- **detect-character-encoding** [ğŸ“](./detect-character-encoding) [ğŸŒ](https://github.com/GerHobbelt/detect-character-encoding) -- detect character encoding using [ICU](http://site.icu-project.org).  **Tip:** If you donâ€™t need ICU in particular, consider using [ced](https://github.com/sonicdoe/ced), which is based on Googleâ€™s lighter [compact_enc_det](https://github.com/google/compact_enc_det) library.
- **enca** [ğŸ“](./enca) [ğŸŒ](https://github.com/GerHobbelt/enca) -- Enca (Extremely Naive Charset Analyser) consists of two main components: `libenca`, an encoding detection library, and `enca`, a command line frontend, integrating libenca and several charset conversion libraries and tools (GNU `recode`, UNIX98 `iconv`, perl `Unicode::Map`, `cstocs`).
- **fastBPE** [ğŸ“](./fastBPE) [ğŸŒ](https://github.com/GerHobbelt/fastBPE) -- text tokenization / ngrams
- **fastText** [ğŸ“](./fastText) [ğŸŒ](https://github.com/GerHobbelt/fastText) -- [fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification. Includes language detection feeatures.
- **glyph_name** [ğŸ“](./glyph_name) [ğŸŒ](https://github.com/GerHobbelt/glyph_name) -- a library for computing Unicode sequences from glyph names according to the Adobe Glyph Naming convention: https://github.com/adobe-type-tools/agl-specification
- **libchardet** [ğŸ“](./libchardet) [ğŸŒ](https://github.com/GerHobbelt/libchardet) -- is based on Mozilla Universal Charset Detector library and, detects the character set used to encode data.
- **libchopshop** [ğŸ“](./libchopshop) [ğŸŒ](https://github.com/GerHobbelt/libchopshop) -- NLP/text processing with automated stop word detection and stemmer-based filtering. This library / toolkit is engineered to be able to provide **both** of the (often more or less disparate) n-gram token streams / vectors required for (1) initializing / training FTS databases, neural nets, etc. and (2) executing effective queries / matches on these engines.
- **libcppjieba** [ğŸ“](./libcppjieba) [ğŸŒ](https://github.com/GerHobbelt/libcppjieba) -- source code extracted from the [CppJieba] project to form a separate project, making it easier to understand and use.
- **libiconv** [ğŸ“](./libiconv) [ğŸŒ](https://github.com/GerHobbelt/libiconv-win-build) -- provides conversion between many platform, language or country dependent character encodings to & from Unicode. This library provides an `iconv()` implementation, for use on systems which don't have one, or whose implementation cannot convert from/to Unicode. It provides support for the encodings: European languages (ASCII, ISO-8859-{1,2,3,4,5,7,9,10,13,14,15,16}, KOI8-R, KOI8-U, KOI8-RU, CP{1250,1251,1252,1253,1254,1257}, CP{850,866,1131}, Mac{Roman,CentralEurope,Iceland,Croatian,Romania}, Mac{Cyrillic,Ukraine,Greek,Turkish}, Macintosh), Semitic languages (ISO-8859-{6,8}, CP{1255,1256}, CP862, Mac{Hebrew,Arabic}), Japanese (EUC-JP, SHIFT_JIS, CP932, ISO-2022-JP, ISO-2022-JP-2, ISO-2022-JP-1, ISO-2022-JP-MS), Chinese (EUC-CN, HZ, GBK, CP936, GB18030, EUC-TW, BIG5, CP950, BIG5-HKSCS, BIG5-HKSCS:2004, BIG5-HKSCS:2001, BIG5-HKSCS:1999, ISO-2022-CN, ISO-2022-CN-EXT), Korean (EUC-KR, CP949, ISO-2022-KR, JOHAB), Armenian (ARMSCII-8), Georgian (Georgian-Academy, Georgian-PS), Tajik (KOI8-T), Kazakh (PT154, RK1048), Thai (ISO-8859-11, TIS-620, CP874, MacThai), Laotian (MuleLao-1, CP1133), Vietnamese (VISCII, TCVN, CP1258), Platform specifics (HP-ROMAN8, NEXTSTEP), Full Unicode (UTF-8, UCS-2, UCS-2BE, UCS-2LE, UCS-4, UCS-4BE, UCS-4LE, UTF-16, UTF-16BE, UTF-16LE, UTF-32, UTF-32BE, UTF-32LE, UTF-7, C99, JAVA, UCS-2-INTERNAL, UCS-4-INTERNAL). It also provides support for a few extra encodings: European languages (CP{437,737,775,852,853,855,857,858,860,861,863,865,869,1125}), Semitic languages (CP864), Japanese (EUC-JISX0213, Shift_JISX0213, ISO-2022-JP-3), Chinese (BIG5-2003), Turkmen (TDS565), Platform specifics (ATARIST, RISCOS-LATIN1). It has also some limited support for transliteration, i.e. when a character cannot be represented in the target character set, it can be approximated through one or several similarly looking characters.
- **libnatspec** [ğŸ“](./libnatspec) [ğŸŒ](https://github.com/GerHobbelt/libnatspec) -- The Nation Specifity Library is designed to smooth out national peculiarities when using software. Its primary objectives are: (1) Addressing encoding issues in most popular scenarios, (2) Providing various auxiliary tools that facilitate software localization.
- **libpinyin** [ğŸ“](./libpinyin) [ğŸŒ](https://github.com/GerHobbelt/libpinyin) -- the libpinyin project aims to provide the algorithms core for intelligent sentence-based Chinese pinyin input methods.
- **libpostal** [ğŸ“](./libpostal) [ğŸŒ](https://github.com/GerHobbelt/libpostal) -- a C library for parsing/normalizing street addresses around the world using statistical NLP and open data. The goal of this project is to understand location-based strings in every language, everywhere.
- **libtextcat** [ğŸ“](./libtextcat) [ğŸŒ](https://github.com/GerHobbelt/libtextcat) -- text language detection
- **libunibreak** [ğŸ“](./libunibreak) [ğŸŒ](https://github.com/GerHobbelt/libunibreak) -- an implementation of the line breaking and word breaking algorithms as described in (Unicode Standard Annex 14)[http://www.unicode.org/reports/tr14/] and (Unicode Standard Annex 29)[http://www.unicode.org/reports/tr29/].
- **line_detector** [ğŸ“](./line_detector) [ğŸŒ](https://github.com/GerHobbelt/line_detector) -- line segment detector ([lsd](http://www.ipol.im/pub/art/2012/gjmr-lsd/)) &. edge drawing line detector (edl) &. hough line detector (standard &. probabilistic) for detection.
- **marian** [ğŸ“](./marian) [ğŸŒ](https://github.com/GerHobbelt/marian) -- an efficient Neural Machine Translation framework written in pure C++ with minimal dependencies.
- **open-location-code** [ğŸ“](./open-location-code) [ğŸŒ](https://github.com/GerHobbelt/open-location-code) -- Open Location Code is a technology that gives a way of encoding location into a form that is easier to use than latitude and longitude. The codes generated are called plus codes, as their distinguishing attribute is that they include a "+" character.  The technology is designed to produce codes that can be used as a replacement for street addresses, especially in places where buildings aren't numbered or streets aren't named.  Plus codes represent an area, not a point. As digits are added to a code, the area shrinks, so a long code is more precise than a short code.  Codes that are similar are located closer together than codes that are different.
- **pinyin** [ğŸ“](./pinyin) [ğŸŒ](https://github.com/GerHobbelt/pinyin) -- pÄ«nyÄ«n is a tool for converting Chinese characters to *pinyin*. It can be used for Chinese phonetic notation, sorting, and retrieval.
- **sentencepiece** [ğŸ“](./sentencepiece) [ğŸŒ](https://github.com/GerHobbelt/sentencepiece) -- text tokenization
- **sentence-tokenizer** [ğŸ“](./sentence-tokenizer) [ğŸŒ](https://github.com/GerHobbelt/Tokenizer) -- text tokenization
- **simdutf** [ğŸ“](./simdutf) [ğŸŒ](https://github.com/GerHobbelt/simdutf) -- delivers Unicode validation and transcoding at billions of characters per second, providing fast Unicode functions such as ASCII, UTF-8, UTF-16LE/BE and UTF-32 validation, with and without error identification, Latin1 to UTF-8 transcoding and vice versa, etc.  The functions are accelerated using SIMD instructions (e.g., ARM NEON, SSE, AVX, AVX-512, RISC-V Vector Extension, etc.). When your strings contain hundreds of characters, we can often transcode them at speeds exceeding a billion characters per second. You should expect high speeds not only with English strings (ASCII) but also Chinese, Japanese, Arabic, and so forth. We handle the full character range (including, for example, emojis).
- **uchardet** [ğŸ“](./uchardet) [ğŸŒ](https://github.com/GerHobbelt/uchardet) -- [uchardet](https://www.freedesktop.org/wiki/Software/uchardet/) is an encoding and language detector library, which attempts to determine the encoding of the text. It can reliably detect many charsets. Moreover it also works as a very good and fast language detector.
- **ucto** [ğŸ“](./ucto) [ğŸŒ](https://github.com/GerHobbelt/ucto) -- text tokenization
  
  - **libfolia** [ğŸ“](./libfolia) [ğŸŒ](https://github.com/GerHobbelt/libfolia) -- working with the Format for Linguistic Annotation (FoLiA). Provides a high-level API to read, manipulate, and create FoLiA documents.
  - **uctodata** [ğŸ“](./uctodata) [ğŸŒ](https://github.com/GerHobbelt/uctodata) -- data for `ucto` library

- **uni-algo** [ğŸ“](./uni-algo) [ğŸŒ](https://github.com/GerHobbelt/uni-algo) -- this library handles all unicode conversion/processing problems (there are not only ill-formed sequences actually) properly and always according to The Unicode Standard: in C/C++ there is no safe type for UTF-8/UTF-16 that guarantees that the data will be well-formed; this makes the problem even worse. There are plenty of Unicode libraries for C/C++ out there that implement Unicode algorithms of varying quality, but many of them don't handle ill-formed UTF sequences at all. In the best-case scenario, you'll get an exception/error; in the worst-case, undefined behavior. The biggest problem is that in 99% cases everything will be fine. This is inappropriate for security reasons. This library doesn't work with types/strings/files/streams, it works with the data inside them and makes it safe when it's needed. Check this article if you want more information about ill-formed sequences: https://hsivonen.fi/broken-utf-8 . It is a bit outdated because ICU (International Components for Unicode) now uses W3C conformant implementation too, but the information in the article is very useful. This library does use W3C conformant implementation.
- **unicode-cldr** [ğŸ“](./unicode-cldr) [ğŸŒ](https://github.com/GerHobbelt/cldr) -- Unicode CLDR Project: provides key building blocks for software to support the world's languages, with the largest and most extensive standard repository of locale data available. This data is used by a wide spectrum of companies for their software internationalization and localization, adapting software to the conventions of different languages for such common software tasks.
- **unicode-cldr-data** [ğŸ“](./unicode-cldr-data) [ğŸŒ](https://github.com/GerHobbelt/cldr-json) -- the JSON distribution of CLDR locale data for internationalization. While XML (not JSON) is the "official" format for all CLDR data, this data is programatically generated from the corresponding XML, using the CLDR tooling. This JSON data is generated using only data that has achieved draft="contributed" or draft="approved" status in the CLDR. This is the same threshhold as is used by the ICU (International Components for Unicode).
- **unicode-icu** [ğŸ“](./unicode-icu) [ğŸŒ](https://github.com/GerHobbelt/icu) -- the [International Components for Unicode](https://icu.unicode.org/).
- **unicode-icu-data** [ğŸ“](./unicode-icu-data) [ğŸŒ](https://github.com/GerHobbelt/icu-data) -- International Components for Unicode: Data Repository. This is an auxiliary repository for the International Components for Unicode.
- **unicode-icu-demos** [ğŸ“](./unicode-icu-demos) [ğŸŒ](https://github.com/GerHobbelt/icu-demos) -- ICU Demos contains sample applications built using the International Components for Unicode (ICU) C++ library ICU4C.
- **unicode-inflection** [ğŸ“](./unicode-inflection) [ğŸŒ](https://github.com/GerHobbelt/inflection) -- Code, data and documentation solving language inflection problems. [Inflection](https://en.wikipedia.org/wiki/Inflection) is the process of changing the form of a word to express different grammatical features, such as tense, number, gender, or case. In many languages, inflection is a complex and nuanced process, and it can be difficult to implement inflection correctly in software. This can lead to an inability to express native sounding sentences or to errors in text processing, such as incorrect word forms or incorrect grammatical agreement.
- **unilib** [ğŸ“](./unilib) [ğŸŒ](https://github.com/GerHobbelt/unilib) -- an embeddable C++17 Unicode library.
- **utfcpp** [ğŸ“](./utfcpp) [ğŸŒ](https://github.com/GerHobbelt/utfcpp) -- UTF-8 with C++ in a Portable Way
- **win-iconv** [ğŸ“](./win-iconv) [ğŸŒ](https://github.com/GerHobbelt/win-iconv) -- an `iconv` implementation using Win32 API to convert.
- **worde_butcher** [ğŸ“](./worde_butcher) [ğŸŒ](https://github.com/GerHobbelt/worde_butcher) -- a tool for text segmentation, keyword extraction and speech tagging. Butchers any text into prime word / phrase cuts, deboning all incoming based on our definitive set of stopwords for all languages.
- **xmunch** [ğŸ“](./xmunch) [ğŸŒ](https://github.com/GerHobbelt/xmunch) -- xmunch essentially does, what the 'munch' command of, for example, hunspell does, but is not compatible with hunspell affix definition files. So why use it then? What makes xmunch different from the other tools is the ability to extend incomplete word-lists. For hunspell's munch to identify a stem and add an affix mark, every word formed by the affix with the stem has to be in the original word-list. This makes sense for a compression tool. However if your word list is incomplete, you have to add all possible word-forms of a word first, before any compression is done. Using xmunch instead, you can define a subset of forms which are required to be in the word-list to allow the word to be used as stem. Like this, you can extend the word-list.
- **you-token-to-me** [ğŸ“](./you-token-to-me) [ğŸŒ](https://github.com/GerHobbelt/YouTokenToMe) -- text tokenization
- **ztd.text** [ğŸ“](./ztd.text) [ğŸŒ](https://github.com/GerHobbelt/ztd.text) -- an implementation of an up and coming proposal percolating through SG16, [P1629 - Standard Text Encoding](https://thephd.github.io/_vendor/future_cxx/papers/d1629.html). It will also include implementations of some downstream ideas covered in Previous Work in this area, including Zach Laine's [Boost.Text (proposed)](https://github.com/tzlaine/text), rmf's [libogonek](https://github.com/libogonek/ogonek), and Tom Honermann's [text_view](https://github.com/tahonermann/text_view).











